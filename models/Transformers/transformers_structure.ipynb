{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      " tensor([[-0.9007,  1.2370,  0.9771],\n",
      "        [-0.1645,  3.6467,  0.6665]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义输入维度和输出维度\n",
    "input_dim = 4  # 输入特征数量\n",
    "dim_k = 3      # 输出特征数量\n",
    "\n",
    "# 创建线性层\n",
    "linear_layer = nn.Linear(4, 3)\n",
    "\n",
    "# 创建一个输入张量（假设有两个样本）\n",
    "input_tensor = torch.tensor([[1.0, 2.0, 3.0, 4.0], \n",
    "                              [5.0, 6.0, 7.0, 8.0]])\n",
    "\n",
    "# 前向传播\n",
    "output_tensor = linear_layer(input_tensor)\n",
    "\n",
    "print(\"Output:\\n\", output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape = torch.Size([4, 3, 10])\n",
      "Q shape = torch.Size([2, 4, 3, 2])\n",
      "K shape = torch.Size([2, 4, 3, 2])\n",
      "atten shape = torch.Size([2, 4, 3, 3])\n",
      "matmul shape = torch.Size([2, 4, 3, 2])\n",
      "torch.Size([4, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "class Self_Attention(nn.Module):\n",
    "    # input : batch_size * seq_len * input_dim\n",
    "    # q : batch_size * input_dim * dim_k\n",
    "    # k : batch_size * input_dim * dim_k\n",
    "    # v : batch_size * input_dim * dim_v\n",
    "    def __init__(self,input_dim,dim_k,dim_v):\n",
    "        super(Self_Attention,self).__init__()\n",
    "        self.q = nn.Linear(input_dim,dim_k)\n",
    "        self.k = nn.Linear(input_dim,dim_k)\n",
    "        self.v = nn.Linear(input_dim,dim_v)\n",
    "        self._norm_fact = 1 / sqrt(dim_k)\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        Q = self.q(x) # Q: batch_size * seq_len * dim_k\n",
    "        K = self.k(x) # K: batch_size * seq_len * dim_k\n",
    "        V = self.v(x) # V: batch_size * seq_len * dim_v\n",
    "         \n",
    "        atten = nn.Softmax(dim=-1)(torch.bmm(Q,K.permute(0,2,1))) * self._norm_fact # Q * K.T() # batch_size * seq_len * seq_len\n",
    "        \n",
    "        output = torch.bmm(atten,V) # Q * K.T() * V # batch_size * seq_len * dim_v\n",
    "        \n",
    "        return output\n",
    "\n",
    "class Mutihead_Attention(nn.Module):\n",
    "    def __init__(self,d_model,dim_k,dim_v,n_heads):\n",
    "        super(Mutihead_Attention, self).__init__()\n",
    "        self.dim_v = dim_v\n",
    "        self.dim_k = dim_k\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.q = nn.Linear(d_model,dim_k)\n",
    "        self.k = nn.Linear(d_model,dim_k)\n",
    "        self.v = nn.Linear(d_model,dim_v)\n",
    "\n",
    "        self.o = nn.Linear(dim_v,d_model)\n",
    "        self.norm_fact = 1 / math.sqrt(d_model)\n",
    "\n",
    "    def generate_mask(self,dim):\n",
    "        # 此处是 sequence mask ，防止 decoder窥视后面时间步的信息。\n",
    "        # padding mask 在数据输入模型之前完成。\n",
    "        matirx = np.ones((dim,dim))\n",
    "        mask = torch.Tensor(np.tril(matirx))\n",
    "\n",
    "        return mask==1\n",
    "\n",
    "    def forward(self,x,y,requires_mask=False):\n",
    "        assert self.dim_k % self.n_heads == 0 and self.dim_v % self.n_heads == 0\n",
    "        # size of x : [batch_size * seq_len * batch_size]\n",
    "        # 对 x 进行自注意力\n",
    "        Q = self.q(x).reshape(-1,x.shape[0],x.shape[1],self.dim_k // self.n_heads) # n_heads * batch_size * seq_len * dim_k\n",
    "        K = self.k(x).reshape(-1,x.shape[0],x.shape[1],self.dim_k // self.n_heads) # n_heads * batch_size * seq_len * dim_k\n",
    "        V = self.v(y).reshape(-1,y.shape[0],y.shape[1],self.dim_v // self.n_heads) # n_heads * batch_size * seq_len * dim_v\n",
    "        \n",
    "        # print(\"Attention V shape : {}\".format(V.shape))\n",
    "        attention_score = torch.matmul(Q,K.permute(0,1,3,2)) * self.norm_fact\n",
    "        \n",
    "        if requires_mask:\n",
    "            mask = self.generate_mask(x.shape[1])\n",
    "            # masked_fill 函数中，对Mask位置为True的部分进行Mask\n",
    "            attention_score.masked_fill(mask,value=float(\"-inf\")) # 注意这里的小Trick，不需要将Q,K,V 分别MASK,只MASKSoftmax之前的结果就好了\n",
    "        print(\"V = {}\".format(V.shape))\n",
    "        print(\"attention_score = {}\".format(attention_score.shape))\n",
    "        output1 = torch.matmul(attention_score,V)\n",
    "        print(\"output1 = {}\".format(output1.shape))\n",
    "        output = output1.reshape(y.shape[0],y.shape[1],-1)\n",
    "        print(\"output = {}\".format(output.shape))\n",
    "        output = self.o(output)\n",
    "        print(\"output = {}\".format(output.shape))\n",
    "        return output\n",
    "\n",
    "class Self_Attention_Muti_Head(nn.Module):\n",
    "    # input : batch_size * seq_len * input_dim\n",
    "    # q : batch_size * input_dim * dim_k\n",
    "    # k : batch_size * input_dim * dim_k\n",
    "    # v : batch_size * input_dim * dim_v\n",
    "    def __init__(self,input_dim,dim_k,dim_v,nums_head):\n",
    "        super(Self_Attention_Muti_Head,self).__init__()\n",
    "        assert dim_k % nums_head == 0\n",
    "        assert dim_v % nums_head == 0\n",
    "        self.q = nn.Linear(input_dim,dim_k)\n",
    "        self.k = nn.Linear(input_dim,dim_k)\n",
    "        self.v = nn.Linear(input_dim,dim_v)\n",
    "        \n",
    "        self.nums_head = nums_head\n",
    "        self.dim_k = dim_k\n",
    "        self.dim_v = dim_v\n",
    "        self._norm_fact = 1 / sqrt(dim_k)\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        Q = self.q(x).reshape(-1,x.shape[0],x.shape[1],self.dim_k // self.nums_head) \n",
    "        K = self.k(x).reshape(-1,x.shape[0],x.shape[1],self.dim_k // self.nums_head) \n",
    "        V = self.v(x).reshape(-1,x.shape[0],x.shape[1],self.dim_v // self.nums_head)\n",
    "        print(\"input shape = {}\".format(x.shape))\n",
    "        print(\"Q shape = {}\".format(Q.size()))\n",
    "        print(\"K shape = {}\".format(K.size()))\n",
    "\n",
    "        atten = nn.Softmax(dim=-1)(torch.matmul(Q,K.permute(0,1,3,2))) # Q * K.T() # batch_size * seq_len * seq_len\n",
    "        print(\"atten shape = {}\".format(atten.shape))\n",
    "        output1 = torch.matmul(atten,V)\n",
    "        print(\"matmul shape = {}\".format(output1.shape))\n",
    "\n",
    "        output = output1.reshape(x.shape[0],x.shape[1],-1) # Q * K.T() * V # batch_size * seq_len * dim_v\n",
    "        \n",
    "        return output\n",
    "    \n",
    "x = torch.randn(4,3,10)\n",
    "multi_attention = Self_Attention_Muti_Head(10,4,4,2)\n",
    "res=multi_attention(x)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Config' object has no attribute 'dim_k'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 221\u001b[0m\n\u001b[1;32m    217\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(output)\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m--> 221\u001b[0m t\u001b[38;5;241m=\u001b[39m\u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    223\u001b[0m res\u001b[38;5;241m=\u001b[39mt(x)\n",
      "Cell \u001b[0;32mIn[15], line 206\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, N, vocab_size, output_dim)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39md_model,output_dim)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\u001b[38;5;241m*\u001b[39m\u001b[43m[\u001b[49m\u001b[43mTransformer_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n",
      "Cell \u001b[0;32mIn[15], line 206\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39md_model,output_dim)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\u001b[38;5;241m*\u001b[39m[\u001b[43mTransformer_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N)])\n",
      "Cell \u001b[0;32mIn[15], line 188\u001b[0m, in \u001b[0;36mTransformer_layer.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28msuper\u001b[39m(Transformer_layer, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m Decoder()\n",
      "Cell \u001b[0;32mIn[15], line 145\u001b[0m, in \u001b[0;36mEncoder.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28msuper\u001b[39m(Encoder, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding \u001b[38;5;241m=\u001b[39m Positional_Encoding(config\u001b[38;5;241m.\u001b[39md_model)\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmuti_atten \u001b[38;5;241m=\u001b[39m Mutihead_Attention(config\u001b[38;5;241m.\u001b[39md_model,\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim_k\u001b[49m,config\u001b[38;5;241m.\u001b[39mdim_v,config\u001b[38;5;241m.\u001b[39mn_heads)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward \u001b[38;5;241m=\u001b[39m Feed_Forward(config\u001b[38;5;241m.\u001b[39md_model)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_norm \u001b[38;5;241m=\u001b[39m Add_Norm()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Config' object has no attribute 'dim_k'"
     ]
    }
   ],
   "source": [
    "\n",
    "# @Author:Yifx\n",
    "# @Contact: Xxuyifan1999@163.com\n",
    "# @Time:2021/9/16 20:02\n",
    "# @Software: PyCharm\n",
    "\n",
    "\"\"\"\n",
    "文件说明：\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 6\n",
    "\n",
    "        self.d_model = 20\n",
    "        self.n_heads = 2\n",
    "\n",
    "        assert self.d_model % self.n_heads == 0\n",
    "        dim_k  = self.d_model // self.n_heads\n",
    "        dim_v = self.d_model // self.n_heads\n",
    "\n",
    "\n",
    "\n",
    "        self.padding_size = 30\n",
    "        self.UNK = 5\n",
    "        self.PAD = 4\n",
    "\n",
    "        self.N = 6\n",
    "        self.p = 0.1\n",
    "\n",
    "config = Config()\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self,vocab_size):\n",
    "        super(Embedding, self).__init__()\n",
    "        # 一个普通的 embedding层，我们可以通过设置padding_idx=config.PAD 来实现论文中的 padding_mask\n",
    "        self.embedding = nn.Embedding(vocab_size,config.d_model,padding_idx=config.PAD)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # 根据每个句子的长度，进行padding，短补长截\n",
    "        for i in range(len(x)):\n",
    "            if len(x[i]) < config.padding_size:\n",
    "                x[i].extend([config.UNK] * (config.padding_size - len(x[i]))) # 注意 UNK是你词表中用来表示oov的token索引，这里进行了简化，直接假设为6\n",
    "            else:\n",
    "                x[i] = x[i][:config.padding_size]\n",
    "        x = self.embedding(torch.tensor(x)) # batch_size * seq_len * d_model\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class Positional_Encoding(nn.Module):\n",
    "\n",
    "    def __init__(self,d_model):\n",
    "        super(Positional_Encoding,self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "\n",
    "    def forward(self,seq_len,embedding_dim):\n",
    "        positional_encoding = np.zeros((seq_len,embedding_dim))\n",
    "        for pos in range(positional_encoding.shape[0]):\n",
    "            for i in range(positional_encoding.shape[1]):\n",
    "                positional_encoding[pos][i] = math.sin(pos/(10000**(2*i/self.d_model))) if i % 2 == 0 else math.cos(pos/(10000**(2*i/self.d_model)))\n",
    "        return torch.from_numpy(positional_encoding)\n",
    "\n",
    "\n",
    "class Mutihead_Attention(nn.Module):\n",
    "    def __init__(self,d_model,dim_k,dim_v,n_heads):\n",
    "        super(Mutihead_Attention, self).__init__()\n",
    "        self.dim_v = dim_v\n",
    "        self.dim_k = dim_k\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.q = nn.Linear(d_model,dim_k)\n",
    "        self.k = nn.Linear(d_model,dim_k)\n",
    "        self.v = nn.Linear(d_model,dim_v)\n",
    "\n",
    "        self.o = nn.Linear(dim_v,d_model)\n",
    "        self.norm_fact = 1 / math.sqrt(d_model)\n",
    "\n",
    "    def generate_mask(self,dim):\n",
    "        # 此处是 sequence mask ，防止 decoder窥视后面时间步的信息。\n",
    "        # padding mask 在数据输入模型之前完成。\n",
    "        matirx = np.ones((dim,dim))\n",
    "        mask = torch.Tensor(np.tril(matirx))\n",
    "\n",
    "        return mask==1\n",
    "\n",
    "    def forward(self,x,y,requires_mask=False):\n",
    "        assert self.dim_k % self.n_heads == 0 and self.dim_v % self.n_heads == 0\n",
    "        # size of x : [batch_size * seq_len * batch_size]\n",
    "        # 对 x 进行自注意力\n",
    "        Q = self.q(x).reshape(-1,x.shape[0],x.shape[1],self.dim_k // self.n_heads) # n_heads * batch_size * seq_len * dim_k\n",
    "        K = self.k(x).reshape(-1,x.shape[0],x.shape[1],self.dim_k // self.n_heads) # n_heads * batch_size * seq_len * dim_k\n",
    "        V = self.v(y).reshape(-1,y.shape[0],y.shape[1],self.dim_v // self.n_heads) # n_heads * batch_size * seq_len * dim_v\n",
    "        # print(\"Attention V shape : {}\".format(V.shape))\n",
    "        attention_score = torch.matmul(Q,K.permute(0,1,3,2)) * self.norm_fact\n",
    "        if requires_mask:\n",
    "            mask = self.generate_mask(x.shape[1])\n",
    "            # masked_fill 函数中，对Mask位置为True的部分进行Mask\n",
    "            attention_score.masked_fill(mask,value=float(\"-inf\")) # 注意这里的小Trick，不需要将Q,K,V 分别MASK,只MASKSoftmax之前的结果就好了\n",
    "        output = torch.matmul(attention_score,V).reshape(y.shape[0],y.shape[1],-1)\n",
    "        # print(\"Attention output shape : {}\".format(output.shape))\n",
    "\n",
    "        output = self.o(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Feed_Forward(nn.Module):\n",
    "    \"\"\"\n",
    "        先转成 低维度，通过 relu，再转成高维度。\n",
    "\n",
    "        最后输入输出的维度相同\n",
    "    \"\"\"\n",
    "    def __init__(self,input_dim,hidden_dim=2048):\n",
    "        super(Feed_Forward, self).__init__()\n",
    "        self.L1 = nn.Linear(input_dim,hidden_dim)\n",
    "        self.L2 = nn.Linear(hidden_dim,input_dim)  # 将最后一维的 hidden_dim 转成 input_dim，即修改最后一维的维度。\n",
    "\n",
    "    def forward(self,x):\n",
    "        output = nn.ReLU()(self.L1(x))\n",
    "        output = self.L2(output)\n",
    "        return output\n",
    "\n",
    "class Add_Norm(nn.Module):\n",
    "    \"\"\"\n",
    "    做 batch 维度的归一化\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.dropout = nn.Dropout(config.p)\n",
    "        super(Add_Norm, self).__init__()\n",
    "\n",
    "    def forward(self,x,sub_layer,**kwargs):\n",
    "        sub_output = sub_layer(x,**kwargs)\n",
    "        # print(\"{} output : {}\".format(sub_layer,sub_output.size()))\n",
    "        x = self.dropout(x + sub_output)\n",
    "\n",
    "        layer_norm = nn.LayerNorm(x.size()[1:])\n",
    "        out = layer_norm(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.positional_encoding = Positional_Encoding(config.d_model)\n",
    "        self.muti_atten = Mutihead_Attention(config.d_model,config.dim_k,config.dim_v,config.n_heads)\n",
    "        self.feed_forward = Feed_Forward(config.d_model)\n",
    "\n",
    "        self.add_norm = Add_Norm()\n",
    "\n",
    "\n",
    "    def forward(self,x): # batch_size * seq_len 并且 x 的类型不是tensor，是普通list\n",
    "\n",
    "        x += self.positional_encoding(x.shape[1],config.d_model)\n",
    "        # print(\"After positional_encoding: {}\".format(x.size()))\n",
    "        output = self.add_norm(x,self.muti_atten,y=x)\n",
    "        output = self.add_norm(output,self.feed_forward)\n",
    "\n",
    "        return output\n",
    "\n",
    "# 在 Decoder 中，Encoder的输出作为Query和KEy输出的那个东西。即 Decoder的Input作为V。此时是可行的\n",
    "# 因为在输入过程中，我们有一个padding操作，将Inputs和Outputs的seq_len这个维度都拉成一样的了\n",
    "# 我们知道，QK那个过程得到的结果是 batch_size * seq_len * seq_len .既然 seq_len 一样，那么我们可以这样操作\n",
    "# 这样操作的意义是，Outputs 中的 token 分别对于 Inputs 中的每个token作注意力\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.positional_encoding = Positional_Encoding(config.d_model)\n",
    "        self.muti_atten = Mutihead_Attention(config.d_model,config.dim_k,config.dim_v,config.n_heads)\n",
    "        self.feed_forward = Feed_Forward(config.d_model)\n",
    "        self.add_norm = Add_Norm()\n",
    "\n",
    "    def forward(self,x,encoder_output): # batch_size * seq_len 并且 x 的类型不是tensor，是普通list\n",
    "        # print(x.size())\n",
    "        x += self.positional_encoding(x.shape[1],config.d_model)\n",
    "        # print(x.size())\n",
    "        # 第一个 sub_layer\n",
    "        output = self.add_norm(x,self.muti_atten,y=x,requires_mask=True)\n",
    "        # 第二个 sub_layer\n",
    "        output = self.add_norm(x,self.muti_atten,y=encoder_output,requires_mask=True)\n",
    "        # 第三个 sub_layer\n",
    "        output = self.add_norm(output,self.feed_forward)\n",
    "        return output\n",
    "\n",
    "class Transformer_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer_layer, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x_input,x_output = x\n",
    "        encoder_output = self.encoder(x_input)\n",
    "        decoder_output = self.decoder(x_output,encoder_output)\n",
    "        return (encoder_output,decoder_output)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,N,vocab_size,output_dim):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embedding_input = Embedding(vocab_size=vocab_size)\n",
    "        self.embedding_output = Embedding(vocab_size=vocab_size)\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.linear = nn.Linear(config.d_model,output_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.model = nn.Sequential(*[Transformer_layer() for _ in range(N)])\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x_input , x_output = x\n",
    "        x_input = self.embedding_input(x_input)\n",
    "        x_output = self.embedding_output(x_output)\n",
    "\n",
    "        _ , output = self.model((x_input,x_output))\n",
    "\n",
    "        output = self.linear(output)\n",
    "        output = self.softmax(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "t=Transformer(1, 10000, 10000)\n",
    "x = torch.randn(4,100,1)\n",
    "res=t(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.7578,  0.2109, -0.7031, -0.1797],\n",
      "         [-0.3438, -0.7969,  0.9453,  0.3984],\n",
      "         [-0.0312,  0.5625,  0.2344,  0.8750]],\n",
      "\n",
      "        [[-0.8281,  0.4922,  0.5469, -0.7031],\n",
      "         [-0.9453,  0.7656, -0.0312,  0.9531],\n",
      "         [-0.6484,  0.5078,  0.9766,  0.0781]]], dtype=torch.bfloat16)\n",
      "tensor([[[-0.7578,  0.2109, -0.7031, -0.1797],\n",
      "         [-0.3438, -0.7969,  0.9453,  0.3984],\n",
      "         [-0.0312,  0.5625,  0.2344,  0.8750]],\n",
      "\n",
      "        [[-0.8281,  0.4922,  0.5469, -0.7031],\n",
      "         [-0.9453,  0.7656, -0.0312,  0.9531],\n",
      "         [-0.6484,  0.5078,  0.9766,  0.0781]]], dtype=torch.float16)\n",
      "torch.bfloat16\n",
      "tensor([[[0.0000, 0.2109, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.9453, 0.3984],\n",
      "         [0.0000, 0.5625, 0.2344, 0.8750]],\n",
      "\n",
      "        [[0.0000, 0.4922, 0.5469, 0.0000],\n",
      "         [0.0000, 0.7656, 0.0000, 0.9531],\n",
      "         [0.0000, 0.5078, 0.9766, 0.0781]]], dtype=torch.float16)\n",
      "torch.Size([2, 3, 4])\n",
      "tensor([[[0.0000, 0.2109, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.9453, 0.3984],\n",
      "         [0.0000, 0.5625, 0.2344, 0.8750]],\n",
      "\n",
      "        [[0.0000, 0.4922, 0.5469, 0.0000],\n",
      "         [0.0000, 0.7656, 0.0000, 0.9531],\n",
      "         [0.0000, 0.5078, 0.9766, 0.0781]]], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 精度损失那么一点，没啥事\n",
    "x = 2 * torch.rand([2,3,4], dtype=torch.bfloat16) - 1\n",
    "y = x.to(torch.float16)\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print(x.dtype)\n",
    "relu = nn.ReLU()\n",
    "\n",
    "# relu 是对每个元素进行分析的\n",
    "z=relu(y)\n",
    "\n",
    "print(z)\n",
    "print(z.shape)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1162, 0.6670, 0.1475],\n",
      "         [0.9736, 0.3252, 0.1533]]], dtype=torch.float16)\n",
      "tensor([[[0.1937, 0.0000, 0.0000],\n",
      "         [1.6230, 0.5420, 0.0000]]], dtype=torch.float16)\n",
      "torch.Size([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 精度损失那么一点，没啥事\n",
    "x = 2 * torch.rand([1,2,3], dtype=torch.float16) - 1\n",
    "print(x)\n",
    "dropout =  nn.Dropout(0.4)\n",
    "# dropout 对每个元素进行，会按照 0.4 的比例丢弃元素，剩余元素乘以 1/(1-0.4) \n",
    "y=dropout(x)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n",
      "torch.Size([2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[-1.3416, -0.4472,  0.4472,  1.3416],\n",
      "        [-1.3416, -0.4472,  0.4472,  1.3416]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 精度损失那么一点，没啥事\n",
    "#x = 2 * torch.rand([1,2,3], dtype=torch.float32) - 1\n",
    "x=torch.ones([2,2,3], dtype=torch.float32)\n",
    "print(x)\n",
    "print(x.size()[1:])\n",
    "layer_norm = nn.LayerNorm([2,3])\n",
    "out=layer_norm(x)\n",
    "print(out)\n",
    "print(out.shape)\n",
    "\n",
    "\n",
    "\n",
    "input_tensor = torch.tensor([[1.0, 2.0, 3.0, 4.0],\n",
    "                              [5.0, 6.0, 7.0, 8.0]])\n",
    "layer_norm = nn.LayerNorm(4)\n",
    "output_tensor = layer_norm(input_tensor)\n",
    "\n",
    "print(output_tensor)\n",
    "\n",
    "\"\"\"\n",
    "nn.LayerNorm 看懂了，就是为了做归一化的，就是所选的元素集合的均值为 0，方差为 1\n",
    "如果 nn.LayerNorm([3,4]) 则代表，输入的是 [batch_size,3,4]，即一个 batch 中的数据做归一化\n",
    "如果 nn.LayerNorm([4]) 则代表，输入的是 [batch_size,3,4]，即最后一维做数据的归一化\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1000])\n",
      "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
      "          0.0000e+00,  1.0000e+00],\n",
      "        [ 8.4147e-01,  5.5522e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
      "          1.5963e-08,  1.0000e+00],\n",
      "        [ 9.0930e-01, -3.8347e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
      "          3.1927e-08,  1.0000e+00],\n",
      "        ...,\n",
      "        [ 6.5699e-01,  8.2982e-01,  4.5239e-01,  ...,  1.0000e+00,\n",
      "          1.1174e-07,  1.0000e+00],\n",
      "        [ 9.8936e-01, -3.3935e-03,  9.9067e-01,  ...,  1.0000e+00,\n",
      "          1.2771e-07,  1.0000e+00],\n",
      "        [ 4.1212e-01, -8.3358e-01,  6.7637e-01,  ...,  1.0000e+00,\n",
      "          1.4367e-07,  1.0000e+00]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# position 网络\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "class Positional_Encoding(nn.Module):\n",
    "    def __init__(self,d_model):\n",
    "        super(Positional_Encoding,self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "\n",
    "    def forward(self,seq_len,embedding_dim):\n",
    "        positional_encoding = np.zeros((seq_len,embedding_dim))\n",
    "        for pos in range(positional_encoding.shape[0]):\n",
    "            for i in range(positional_encoding.shape[1]):\n",
    "                positional_encoding[pos][i] = math.sin(pos/(10000**(2*i/self.d_model))) if i % 2 == 0 else math.cos(pos/(10000**(2*i/self.d_model)))\n",
    "        return torch.from_numpy(positional_encoding)\n",
    "\n",
    "pe = Positional_Encoding(1024)\n",
    "out=pe(10, 1000)\n",
    "print(out.shape)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
